{
  "model_size": "large-v3",
  "device": "cuda",
  "compute_type": "float16",
  "language": "zh",
  "initial_prompt": "以下是普通话的句子。",
  "host": "0.0.0.0",
  "port": 5000,
  "network_mode": "lan",
  "workers": 4,
  "timeout": 600,
  "log_level": "INFO",
  "max_concurrent_transcriptions": 16,
  "queue_size": 100,
  "llm": {
    "enabled": false,
    "api_url": "https://api-inference.modelscope.cn/v1/",
    "api_key": "",
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "timeout": 30,
    "max_retries": 2,
    "retry_delay": 1,
    "temperature": 0.3,
    "max_tokens": 2000,
    "system_prompt": "你是一个专业的文本修正助手。你的任务是对语音识别转写的文本(主要是软件开发领域)进行纠错修正。要求：1) 必须保持原文的语言，不要翻译成其他语言；2) 改进标点符号；3) 注意相近读音而导致的语音识别转写错误，修正错别字，请勿修改语法或进行润色修改，即你只修改因为读音相近而导致的识别错误文本；4) 只返回纠错后的文本，不要添加任何解释或说明。"
  },
  "_comments": {
    "model_size": "Model size: tiny, base, small, medium, large-v1, large-v2, large-v3",
    "device": "Device: cpu, cuda, auto",
    "compute_type": "Compute type: int8, float16, float32",
    "language": "Language: zh(Chinese), en(English), ja(Japanese), etc",
    "host": "Listen address: 0.0.0.0(LAN), 127.0.0.1(local only)",
    "port": "Port number",
    "network_mode": "Network mode: lan(LAN), internet(Internet)",
    "workers": "Gunicorn worker processes (recommended: 2-8 for GPU)",
    "timeout": "Request timeout in seconds",
    "log_level": "Logging level: DEBUG, INFO, WARNING, ERROR",
    "max_concurrent_transcriptions": "Maximum concurrent transcription requests",
    "queue_size": "Request queue size for load balancing",
    "llm": {
      "enabled": "Enable LLM service for text polishing and correction",
      "api_url": "LLM API endpoint URL (e.g., http://localhost:8000 for ModelScope, Ollama, etc.)",
      "api_key": "API key for authentication (Bearer token)",
      "model": "Model name to use (e.g. Qwen/Qwen2.5-7B-Instruct etc.)",
      "timeout": "LLM request timeout in seconds",
      "max_retries": "Maximum retry attempts for LLM API",
      "retry_delay": "Delay between retries in seconds",
      "temperature": "Sampling temperature (0.0-1.0): lower for more deterministic output",
      "max_tokens": "Maximum tokens in LLM response",
      "system_prompt": "System prompt to guide the LLM behavior"
    }
  }
}